---
layout: post
title: CS229 Lecture5 정리
use_math: true
---

# CS229 Lec 5 정리   

### Lec 5 요약
* Gaussian Discriminative Analysis (GDA)
* Generative vs Discriminative comparison
* Naive Bayes - ex)spam filter <br/>

각 주제들에 대해 간략하게 언급하고 강의에 대해 정리해보도록 하겠습니다.<br/>
여태까지 우리가 배운 것은 discriminative learning algorithms입니다.<br/>
Discriminative learning algorithm = Learn p(y|x) or learn $h_{\theta}(x)$ = 0 or 1 directly (x를 y에 바로 mapping함으로써 나오는 값입니다.)<br/>
Generative learning algorithm = maximize likelihood (이전에 배웠던 separation 해줬던 것들과는 다릅니다.)<br/>
Generative learning algorithm = Learn p(x|y) = class에 주어진 feature들이 어떻게 생겼는지 학습하는 것입니다.(종양을 예시로 드는데, 만약에 악성 종양이면 그 종양의 특성은 어떤 것이 주어져있는지 학습한다는 뜻입니다.)<br/>
Bayes rule <br/>
이것을 적용하면 p(y=1|x) = p(x|y=1)p(y=1) / p(x)로 계산할 수 있습니다.<br/>
ex) p(x) = p(x|y=1)p(y=1) + p(x|y=0)p(y=0)<br/>
- - -

__Gaussian Discriminative Analysis__ (GDA)
• Suppose $x \in R^n$ (drop $x_{0} = 1$ convention) 이렇게 해주고 시작할 것입니다.<br/>
이렇게 해주면 x가 0 ~ n까지 있어도 $x_{0} = 1$이므로 x는 1 ~ n 즉, n개 존재하는 것입니다.<br/>
• Assume p(x|y) is Gaussian -> 종양을 예시로 들면, 악성/양성 종양 모두 가우시안 분포를 가진다는 뜻입니다.<br/>
z ~ N($\mu, \sum)$ <br/>
$mean = E[X] = \mu$ <br/>
$Cov(z) = E[(Z - E[Z])][(Z - E[Z])^T] = E[ZZ^T] - (E[Z])(E[Z])^T = \sum가 됩니다.<br/>

![image](https://user-images.githubusercontent.com/76681022/213909527-8a4c922a-1ba5-4e08-942b-d7ae5b20d699.png)

위의 식은 Multivariate Gaussian Density Function에 대한 설명이고 해당 식은 2개의 parameter를 가지고 있습니다. ($\mu, \sum$)<br/>
(아마 다들 식에 대해서 낯설고 아직 어색할텐데 많이 쓰다보면 익숙해질 것이고, 익숙해지는게 좋을 것 같다고 하셔서 식을 눈에 익혀야 겠다는 생각이 들었습니다.)<br/>

위의 식을 바탕으로 Multivariate Gaussian Density Function에 대한 그림을 보여드리도록 하겠습니다.<br/>
아직 이것이 완벽하게 이해가 가는 것은 아니지만, 눈으로 보고 조금이라도 익숙해져보고자 그림을 올리도록 하였습니다.<br/>
Multi GDF에는 standard normal distribution이 존재합니다.<br/>
mean = 0 covariance는 2x2의 identity matrix로 각 nxn의 원소가 1을 가지는 것을 의미합니다.<br/>

__*<행렬의 수식이 2x2 행렬인데, 블로그에서 표기가 되지 않아서 4x1 행렬로 표현되었습니다. 양해 부탁드립니다.>*__<br/>

![image](https://user-images.githubusercontent.com/76681022/213909678-54a4f909-bd3b-40ca-9602-dd5073d973a5.png)

왼쪽부터 차례대로 $\sum$는 
$\begin{bmatrix} 1 & 0 
\\ 0 & 1 \end{bmatrix}, 
$\begin{bmatrix} 0.6 & 0 
\\ 0 & 0.6 \end{bmatrix}, 
$\begin{bmatrix} 2 & 0 
\\ 0 & 2 \end{bmatrix}$입니다.<br/>

![image](https://user-images.githubusercontent.com/76681022/213909857-ce7dbaf3-d8b2-4be6-9c03-36f31cc6f1a6.png)

왼쪽부터 차례대로 $\sum$는 $\begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix}, $\begin{bmatrix} 1 & 0.5 \\ 0.5 & 1 \end{bmatrix}, $\begin{bmatrix} 1 & 0.8 \\ 0.8 & 1 \end{bmatrix}$입니다.<br/>

바로 위의 그림에 대한 contour들을 그려보면 아래의 그림이 나오게 됩니다.<br/>
![image](https://user-images.githubusercontent.com/76681022/213909911-6015cd5f-fcd3-4889-a8d9-89758b884239.png)

![image](https://user-images.githubusercontent.com/76681022/213909922-e689ab2e-7279-4b14-a167-5285599c3059.png)

왼쪽부터 차례대로 $\sum$는 $\begin{bmatrix} 1 & -0.5 \\ -0.5 & 1 \end{bmatrix}, $\begin{bmatrix} 1 & -0.8 \\ -0.8 & 1 \end{bmatrix}, $\begin{bmatrix} 3 & 0.8 \\ 0.8 & 1 \end{bmatrix}$입니다.<br/>
여기까지의 그림들은 covariance를 조작하여 얻은 그림들이고 아래의 그림부터는 mean을 변화시켜서 얻은 그림입니다.<br/>

![image](https://user-images.githubusercontent.com/76681022/213909979-84b6222f-3f4e-4b8d-8f61-8e82b1e1d954.png)

왼쪽부터 차례대로 $\mu$는 $\begin{bmatrix} 1 \\ 0 \end{bmatrix}, $\begin{bmatrix} -0.5 \\ 0 \end{bmatrix}, $\begin{bmatrix} -1 \\ -1.5 \end{bmatrix}$입니다.<br/>

위의 그림들을 mean과 covariance에 맞게 찬찬히 살펴보면 대충의 Multi GDF에 대한 대충의 느낌은 잡을 수 있습니다.<br/>
- - -

__Gaussian Distribution Analysis(GDA)__<br/>
GDA model은 다음과 같습니다.<br/>

![image](https://user-images.githubusercontent.com/76681022/213910280-f846f5a3-95cb-42db-9597-abce6339d137.png)
![image](https://user-images.githubusercontent.com/76681022/213910288-cde2a2e2-33a9-4437-83b3-5545747bcd16.png)

P(y) -> Bernoulli distribution (binary classification이므로)<br/>
P(x|y=0) -> Assume density of features is Gaussian.(양성 종양일때의 feature들이 어떻게 생겼는지를 의미합니다.)<br/>
P(x|y=1) -> Assume density of features is Gaussian <br/>
GDA Model의 parameter는 $\mu_{0}, \mu_{1}, \sum, \phi$입니다.($\mu_{0}, \mu_{1} \in R^n, \sum \in R^{nxn}, \phi \in R$)<br/>
둘 다 $\sum$는 같은 것을 쓰지만 $\mu$는 다른 것을 사용합니다 => 일반적으로 둘 다 같은 covariance를 가지지만, 다른 mean을 가진다고 가정할 것입니다. <br/>
이 parameter들을 fit 해주기 위해서는 maximize the joint likelihood를 해주어야 합니다.<br/>
위의 maximize the joint likelihood를 해주려면 아래의 과정을 거쳐야합니다.<br/>
(이제부터는 training set을 ${ (x^i, y^i)_{i=1}^m}$ 이라고 할 것입니다.)<br/>

![image](https://user-images.githubusercontent.com/76681022/213910468-d2930e04-04a8-49d4-92c6-2a7e24becaf6.png)

Discriminative algorithm (linear regression, logistic regression, GLM 같은 것들)은 아래의 식과 같이 조건부 확률(conditional likelihood)을 maximize 해줘야합니다.<br/>
즉, $\theta$를 선택하여 p(y|x)를 maximize 해줘야 합니다. <br/>

![image](https://user-images.githubusercontent.com/76681022/213910211-5b53ace6-29e5-4bdb-a758-cf885faabcd0.png)

하지만 generative learning algorithm은 parameter를 선택해서 p(x,y)를 maximize 해줘야 합니다.<br/>
Generative algorithm이 discriminative algorithm과 다른 것은 cost function이 x,y로 이루어진 joint likelihood를 maximize 해주어야 한다는 것입니다. <br/>

만약에 MLE(Maximum Likelihood Estimation)를 사용한다고 하였을 때 parameter $\mu_{0}, \mu_{1}, \sum, \phi$를 선택해서 maximize log $L(\mu_{0},\mu_{1}, \sum, \phi$)를 해줘야 합니다.<br/>
(Maximize 해주려면 위 generative learning algorithm의 log-likelihood 식의 형태를 보고 미분해서 0이 되는 값을 구해주면 그 지점의 parameter들의 값이 maximize해줄 수 있는 값 입니다.)<br/>
아래의 식들이 해당 값 입니다. (아래의 식에서 1{$y^i = 1$}은 indicator function으로 해당 1{ }안의 값이 해당 집합에 속하면 1 아니면 0으로 해주는 함수입니다.)<br/>

![image](https://user-images.githubusercontent.com/76681022/213910570-0c127642-a5b9-4039-a5e7-4eabb3ab284b.png)

위의 식들은 직관적으로 쉽게 이해하려면 동전 던지기 예시를 들어 설명할 수 있습니다.<br/>
하지만 log likelihood식을 미분하면서 직접 수학적으로 풀어보는 것이 좀 더 formal한 증명이 될 것 같습니다.<br/>
그리고 위의 식에서 $\mu_{0}$에서 분자는 y=0일 때 feature vector들의 합이고, 분모는 y=0인 example들의 합입니다.<br/>
강의에서 설명하기로는 covariance는 각 y=0 / y=1의 분포를 등고선으로 나타내는 것들의 곱이라고 하였습니다만, 저희는 이것을 하나의 covariance matrix로 나타내어야 합니다.<br/>
(이것은 제가 확률에 대해 공부가 미흡한 탓인지 잘 이해가 되지 않아 추후에 좀 더 자세하게 알아보도록 하겠습니다.)<br/>

- - -

__Generative learning algorithm vs Discriminative learning algorithm__ <br/>
![image](https://user-images.githubusercontent.com/76681022/213910679-80226c69-b184-4b55-9f1b-4c59827346cd.png)
P(x) = constant 이므로 argmax를 구할 때는 계산을 줄이기 위해 빼고 계산할 수도 있습니다.<br/>

![5-1](https://user-images.githubusercontent.com/76681022/213910723-f4bd0596-8a67-4fb8-890f-0ab977fecda3.png)

파란선은 Bayes’ rule을 사용해서 decision boundary를 구분한 것이고, 초록색 선은 logistic regression을 사용해서 decision boundary를 구분해준 것입니다.<br/>
위의 그림을 보고 generative learning algorithm 과 discriminative learning algorithm을 비교해보도록 하겠습니다.<br/>
Logistic regression -> O,X에 해당하는 예시들을 구분하는 직선을 그어주는 것입니다.<br/>
Generative learning algorithm(GDA 사용 = fit gaussian density function) -> 2개의 class들을 구분되게 보여주었는데, 같은 covariance를 쓰기 때문입니다. 그래서 완전 구분해서 볼 수는 없습니다.<br/>
결국 그림을 보면 두개는 완전 같은 것이 아니라, 조금이라도 다른 것임을 알 수 있습니다. <br/>

강의 중 질문에서 저도 궁금했던 것이 있어 이에 대한 답변도 같이 올려보겠습니다.<br/>
Q : 왜 2개의 covariance를 쓰지 않나요??<br/>
A : 2개를 쓸 수도 있지만, 그렇게 하면 decision boundary가 더 이상 linear하지 않게되고, 계산이 더 복잡해집니다. 그리고 변수를 2개 사용하는 것이므로 더 복잡해진다. 써도 안되는 것은 아닙니다.<br/>
- - -

__GDA vs Logistic regression__ <br/>
* GDA(generative approach) <br/>
Assume <br/>
(x|y=0) ~ N($\mu_{0}, \sum$)<br/>
(x|y=1) ~ N($\mu_{1}, \sum$)<br/>
Y ~ Bernoulli($\phi$)<br/>
* Logistic regression <br/>
P(y=1|x)<br/>
GDA가 stronger assumptions으로 GDA면 Logistic regression의 가정을 쓸 수 있지만 Logistic regression은 GDA를 쓸 수는 없습니다.<br/>
(x|y=0, x|y=1이 exponential family의 distribution을 따르고 둘 다 natural parameter를 가질 때, y=1|x는 logistic regression일 것이라는 뜻입니다.)<br/>
위의 GDA처럼 stronger assumptions을 만들면 data-set에 대해 예측도 더 잘하고 많은 정보를 말할 수 있습니다.<br/>
하지만 반대로 위의 가정이 틀릴 경우에 오히려 더 나쁜 결과를 낼 수 있습니다.<br/>
data의 분포가 Gaussian이 아니고 Poisson일 경우, 오히려 Logistic regression(weaker assumptions)을 사용하는게 더 modeling에 있어서 robust 할 수 있습니다.<br/>
하지만 data-set이 작은 경우에, GDA처럼 오히려 더 세세하게 가정을 해주고 modeling을 해주는 것이 더 나은 결과를 만들어줄 수도 있습니다.<br/>
그래서 만약 더 작은 data-set이 있는 경우에, ML에 대한 지식과 스킬이 더 중요하다고 말할 수 있습니다.<br/>
왜냐하면 distribution을 직접 설정해주고 더 세세한 설정을 해줄수록 더 성능이 좋아지기 때문입니다.<br/>
그래서 업무에서 실력이 있는 사람들과 실력이 없는 사람들이 현저히 차이가 날 것이라고 생각이 됩니다.<br/>
- - -

__Naive Bayes__ <br/>
또 다른 generative learning algorithm인 Naive Bayes에 대해 알아보겠습니다.<br/>
먼저 예시로 ML을 활용한 spam-filter를 만드는 것을 들어보겠습니다.<br/>
이메일이 왔을 때, 특정 단어를 인식해서 그 단어가 포함되어 있으면 스팸메일로 인식할 것입니다.<br/>
그렇게 하기 위해서는 특정 단어를 인식해야 하는데, 그 특정 단어를 feature vector를 이용해서 0,1로 구분하여 인식할 것입니다.<br/>
아래의 그림의 feature vector에 50,000개의 단어가 있다고 가정해보겠습니다.<br/>
그렇게되면 $x \in {0,1}^50,000$이 될 것입니다.<br/>
$x_{i} = 1{word i appears in email}$ <- indicator function을 사용해서 해당 단어가 이메일에 있으면 1, 없으면 0의 값을 가지게 될 것입니다.$<br/>
![image](https://user-images.githubusercontent.com/76681022/214243367-7ec35d53-78b6-4480-9e58-461a4acbff7b.png)

저희가 modeling하기를 원하는 것은 p(x|y), p(y)입니다.<br/>
그려면 x에 대해서는 $2^50,000$가지의 경우가 존재할 것입니다.<br/>
그렇게 되면 $2^50,000 - 1$개의 parameter가 필요하게될 것입니다.<br/>
결국 연산량이 너무 많아지게 됩니다.<br/>
따라서 $x_{i}가 conditionally independent given y$라고 가정을 해보겠습니다.<br/>
(이메일에 a라는 단어가 나오면 이것은 aardvark라는 단어가 나온것에 영향을 받지 않는다는 뜻입니다.)<br/>
그러면 아래의 식처럼 나오게 됩니다.<br/>
(이것은 probability의 chain rule을 사용한 것입니다)
![image](https://user-images.githubusercontent.com/76681022/214243689-4b4b5953-eee8-4eb3-9c72-cd4530655d69.png)

이 스팸메일을 걸러주는 모델을 만들기 위해서 필요한 parameter들은 $\phi_{j|y = 1}, \phi_{j|y = 0}, \phi_{y}$가 있습니다.<br/>
각각의 parameter들은 $\phi_{j|y = 1} = p(x_{j}=1| y=1), \phi_{j|y = 0} = p(x_{j}=1| y=0), \phi_{y} = p(y=1)$입니다.<br/>
위의 parameter들을 fit해주기 위해서 Joint Likelihood 식을 사용해야합니다.<br/>
이렇게해서 스팸메일을 걸러주는 것에 대한 장단점이 존재하는데 Lec 6에서 더 알아보도록 하겠습니다.<br/>
![image](https://user-images.githubusercontent.com/76681022/214244591-7783674a-1c68-4b72-bc34-40d0a1fe1451.png)




