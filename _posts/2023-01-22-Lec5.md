---
layout: post
title: CS229 Lecture5 정리
use_math: true
---

# CS229 Lec 5 정리   

### Lec 5 요약
* Gaussian Discriminative Analysis (GDA)
* Generative vs Discriminative comparison
* Naive Bayes - ex)spam filter <br/>

각 주제들에 대해 간략하게 언급하고 강의에 대해 정리해보도록 하겠습니다.<br/>
여태까지 우리가 배운 것은 discriminative learning algorithms입니다.<br/>
Discriminative learning algorithm = Learn p(y|x) or learn $h_{\theta}(x)$ = 0 or 1 directly (x를 y에 바로 mapping함으로써 나오는 값입니다.)<br/>
Generative learning algorithm = maximize likelihood (이전에 배웠던 separation 해줬던 것들과는 다릅니다.)<br/>
Generative learning algorithm = Learn p(x|y) = class에 주어진 feature들이 어떻게 생겼는지 학습하는 것입니다.(종양을 예시로 드는데, 만약에 악성 종양이면 그 종양의 특성은 어떤 것이 주어져있는지 학습한다는 뜻입니다.)<br/>
Bayes rule <br/>
이것을 적용하면 p(y=1|x) = p(x|y=1)p(y=1) / p(x)로 계산할 수 있습니다.<br/>
ex) p(x) = p(x|y=1)p(y=1) + p(x|y=0)p(y=0)<br/>
- - -

__Gaussian Discriminative Analysis__ (GDA)
• Suppose $x \in R^n$ (drop $x_{0} = 1$ convention) 이렇게 해주고 시작할 것입니다.<br/>
이렇게 해주면 x가 0 ~ n까지 있어도 $x_{0} = 1$이므로 x는 1 ~ n 즉, n개 존재하는 것입니다.<br/>
• Assume p(x|y) is Gaussian -> 종양을 예시로 들면, 악성/양성 종양 모두 가우시안 분포를 가진다는 뜻입니다.<br/>
z ~ N($\mu, \sum)$ <br/>
$mean = E[X] = \mu$ <br/>
$Cov(z) = E[(Z - E[Z])][(Z - E[Z])^T] = E[ZZ^T] - (E[Z])(E[Z])^T = \sum가 됩니다.<br/>

![image](https://user-images.githubusercontent.com/76681022/213909527-8a4c922a-1ba5-4e08-942b-d7ae5b20d699.png)

위의 식은 Multivariate Gaussian Density Function에 대한 설명이고 해당 식은 2개의 parameter를 가지고 있습니다. ($\mu, \sum$)<br/>
(아마 다들 식에 대해서 낯설고 아직 어색할텐데 많이 쓰다보면 익숙해질 것이고, 익숙해지는게 좋을 것 같다고 하셔서 식을 눈에 익혀야 겠다는 생각이 들었습니다.)<br/>

위의 식을 바탕으로 Multivariate Gaussian Density Function에 대한 그림을 보여드리도록 하겠습니다.<br/>
아직 이것이 완벽하게 이해가 가는 것은 아니지만, 눈으로 보고 조금이라도 익숙해져보고자 그림을 올리도록 하였습니다.<br/>
Multi GDF에는 standard normal distribution이 존재합니다.<br/>
mean = 0 covariance는 2x2의 identity matrix로 각 nxn의 원소가 1을 가지는 것을 의미합니다.<br/>

__행렬의 수식이 2x2 행렬인데, 블로그에서 표기가 되지 않아서 4x1 행렬로 표현되었습니다. 양해 부탁드립니다.__<br/>

![image](https://user-images.githubusercontent.com/76681022/213909678-54a4f909-bd3b-40ca-9602-dd5073d973a5.png)

왼쪽부터 차례대로 $\sum$는 
$\begin{bmatrix} 1 & 0 
\\ 0 & 1 \end{bmatrix}, 
$\begin{bmatrix} 0.6 & 0 
\\ 0 & 0.6 \end{bmatrix}, 
$\begin{bmatrix} 2 & 0 
\\ 0 & 2 \end{bmatrix}$입니다.<br/>

![image](https://user-images.githubusercontent.com/76681022/213909857-ce7dbaf3-d8b2-4be6-9c03-36f31cc6f1a6.png)

왼쪽부터 차례대로 $\sum$는 $\begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix}, $\begin{bmatrix} 1 & 0.5 \\ 0.5 & 1 \end{bmatrix}, $\begin{bmatrix} 1 & 0.8 \\ 0.8 & 1 \end{bmatrix}$입니다.<br/>

바로 위의 그림에 대한 contour들을 그려보면 아래의 그림이 나오게 됩니다.<br/>
![image](https://user-images.githubusercontent.com/76681022/213909911-6015cd5f-fcd3-4889-a8d9-89758b884239.png)

![image](https://user-images.githubusercontent.com/76681022/213909922-e689ab2e-7279-4b14-a167-5285599c3059.png)

왼쪽부터 차례대로 $\sum$는 $\begin{bmatrix} 1 & -0.5 \\ -0.5 & 1 \end{bmatrix}, $\begin{bmatrix} 1 & -0.8 \\ -0.8 & 1 \end{bmatrix}, $\begin{bmatrix} 3 & 0.8 \\ 0.8 & 1 \end{bmatrix}$입니다.<br/>
여기까지의 그림들은 covariance를 조작하여 얻은 그림들이고 아래의 그림부터는 mean을 변화시켜서 얻은 그림입니다.<br/>

![image](https://user-images.githubusercontent.com/76681022/213909979-84b6222f-3f4e-4b8d-8f61-8e82b1e1d954.png)

왼쪽부터 차례대로 $\mu$는 $\begin{bmatrix} 1 \\ 0 \end{bmatrix}, $\begin{bmatrix} -0.5 \\ 0 \end{bmatrix}, $\begin{bmatrix} -1 \\ -1.5 \end{bmatrix}$입니다.<br/>

위의 그림들을 mean과 covariance에 맞게 찬찬히 살펴보면 대충의 Multi GDF에 대한 대충의 느낌은 잡을 수 있습니다.<br/>
- - -

__Gaussian Distribution Analysis(GDA)__<br/>
GDA model은 다음과 같습니다.<br/>

![image](https://user-images.githubusercontent.com/76681022/213910280-f846f5a3-95cb-42db-9597-abce6339d137.png)
![image](https://user-images.githubusercontent.com/76681022/213910288-cde2a2e2-33a9-4437-83b3-5545747bcd16.png)

P(y) -> Bernoulli distribution (binary classification이므로)<br/>
P(x|y=0) -> Assume density of features is Gaussian.(양성 종양일때의 feature들이 어떻게 생겼는지를 의미합니다.)<br/>
P(x|y=1) -> Assume density of features is Gaussian <br/>
GDA Model의 parameter는 $\mu_{0}, \mu_{1}, \sum, \phi$입니다.($\mu_{0}, \mu_{1} \in R^n, \sum \in R^{nxn}, \phi \in R$)<br/>
둘 다 $\sum$는 같은 것을 쓰지만 $\mu$는 다른 것을 사용합니다 => 일반적으로 둘 다 같은 covariance를 가지지만, 다른 mean을 가진다고 가정할 것입니다. <br/>
이 parameter들을 fit 해주기 위해서는 maximize the joint likelihood를 해주어야 합니다.<br/>
위의 maximize the joint likelihood를 해주려면 아래의 과정을 거쳐야합니다.<br/>
(이제부터는 training set을 ${ (x^i, y^i)_{i=1}^m}$ 이라고 할 것입니다.)<br/>

![image](https://user-images.githubusercontent.com/76681022/213910468-d2930e04-04a8-49d4-92c6-2a7e24becaf6.png)

Discriminative algorithm (linear regression, logistic regression, GLM 같은 것들)은 아래의 식과 같이 조건부 확률(conditional likelihood)을 maximize 해줘야합니다.<br/>
즉, $\theta$를 선택하여 p(y|x)를 maximize 해줘야 합니다. <br/>

![image](https://user-images.githubusercontent.com/76681022/213910211-5b53ace6-29e5-4bdb-a758-cf885faabcd0.png)

하지만 generative learning algorithm은 parameter를 선택해서 p(x,y)를 maximize 해줘야 합니다.<br/>
Generative algorithm이 discriminative algorithm과 다른 것은 cost function이 x,y로 이루어진 joint likelihood를 maximize 해주어야 한다는 것입니다. <br/>

만약에 MLE(Maximum Likelihood Estimation)를 사용한다고 하였을 때 parameter $\mu_{0}, \mu_{1}, \sum, \phi$를 선택해서 maximize log $L(\mu_{0},\mu_{1}, \sum, \phi$)를 해줘야 합니다.<br/>
(Maximize 해주려면 위 generative learning algorithm의 log-likelihood 식의 형태를 보고 미분해서 0이 되는 값을 구해주면 그 지점의 parameter들의 값이 maximize해줄 수 있는 값 입니다.)<br/>
아래의 식들이 해당 값 입니다. (아래의 식에서 1{$y^i = 1$}은 indicator function으로 해당 1{ }안의 값이 해당 집합에 속하면 1 아니면 0으로 해주는 함수입니다.)<br/>

![image](https://user-images.githubusercontent.com/76681022/213910570-0c127642-a5b9-4039-a5e7-4eabb3ab284b.png)

위의 식들은 직관적으로 쉽게 이해하려면 동전 던지기 예시를 들어 설명할 수 있습니다.<br/>
하지만 log likelihood식을 미분하면서 직접 수학적으로 풀어보는 것이 좀 더 formal한 증명이 될 것 같습니다.<br/>
그리고 위의 식에서 $\mu_{0}$에서 분자는 y=0일 때 feature vector들의 합이고, 분모는 y=0인 example들의 합입니다.<br/>
강의에서 설명하기로는 covariance는 각 y=0 / y=1의 분포를 등고선으로 나타내는 것들의 곱이라고 하였습니다만, 저희는 이것을 하나의 covariance matrix로 나타내어야 합니다.<br/>
(이것은 제가 확률에 대해 공부가 미흡한 탓인지 잘 이해가 되지 않아 추후에 좀 더 자세하게 알아보도록 하겠습니다.)<br/>

- - -

__Generative learning algorithm vs Discriminative learning algorithm__ <br/>
![image](https://user-images.githubusercontent.com/76681022/213910679-80226c69-b184-4b55-9f1b-4c59827346cd.png)
P(x) = constant 이므로 argmax를 구할 때는 계산을 줄이기 위해 빼고 계산할 수도 있습니다.<br/>

![5-1](https://user-images.githubusercontent.com/76681022/213910723-f4bd0596-8a67-4fb8-890f-0ab977fecda3.png)

파란선은 Bayes’ rule을 사용해서 decision boundary를 구분한 것이고, 초록색 선은 logistic regression을 사용해서 decision boundary를 구분해준 것입니다.<br/>
위의 그림을 보고 generative learning algorithm 과 discriminative learning algorithm을 비교해보도록 하겠습니다.<br/>
Logistic regression -> O,X에 해당하는 예시들을 구분하는 직선을 그어주는 것입니다.<br/>
Generative learning algorithm(GDA 사용 = fit gaussian density function) -> 2개의 class들을 구분되게 보여주었는데, 같은 covariance를 쓰기 때문입니다. 그래서 완전 구분해서 볼 수는 없습니다.<br/>
결국 그림을 보면 두개는 완전 같은 것이 아니라, 조금이라도 다른 것임을 알 수 있습니다. <br/>

강의 중 질문에서 저도 궁금했던 것이 있어 이에 대한 답변도 같이 올려보겠습니다.<br/>
Q : 왜 2개의 covariance를 쓰지 않나요??<br/>
A : 2개를 쓸 수도 있지만, 그렇게 하면 decision boundary가 더 이상 linear하지 않게되고, 계산이 더 복잡해집니다. 그리고 변수를 2개 사용하는 것이므로 더 복잡해진다. 써도 안되는 것은 아닙니다.<br/>

__GDA vs Logistic regression__ <br/>

...
