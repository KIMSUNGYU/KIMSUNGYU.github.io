---
layout: post
title: CS229 Lecture4 정리
use_math: true
---

# CS229 Lec 4 정리   

### Lec 4 요약
Perceptron Learning algorithm, Exponential Familiy, Generalized Linear Models, Softmax Regression, Multiclass Classification 에 대해 정리해보겠습니다.

__Perceptron Learning algorithm__<br/>
먼저 앞선 강의에서 저희는 $\theta_{j} = \theta_{j} + \alpha(y^i - h_{\theta}(x^i))x_{j}^i$와 같은 learning algorithm을 배웠었습니다.<br/>
이것을 Perceptron learning algorithm이라고 합니다.<br/>
이번 강의에서는 좌표평면위의 그래프를 통해서 어떻게 $\theta^TX$와 $\alpha$가 어떻게 학습을 통해서 개선되는지 눈으로 보면서 좀 더 이해를 쉽게 할 수 있었습니다.<br/>
O,X 2개의 class가 존재하는 그래프에서 예시를 보여주고, 새로운 data가 기존의 학습 방법에 따라 mis-classified되면 기존의 판별 기준을 수정해야합니다.<br/>
근데 이것을 직접 선을 그려서 $\theta$와 x에 대한 그래프로 그려서 learning rate를 적용해서 새로 학습하게되면 learning algorithm이 어떻게 개선되는지 직접 눈으로 봐서 그저 되겠구나가 아니라 vector관점으로 납득할 수 있었습니다. ($\theta$가 learning rate를 적용한 x와 더해지면서 개선된 $\theta$가 나오게 됨.)<br/>
하지만 이것을 실제로 잘 사용되지 않는 이유는 Vector 관점에서는 해석이 가능하지만 Probablistic interpretation이 가능하지 않고 XOR Problem 같은 것을 해결하지 못하는 한계가 존재하기 때문입니다.<br/>

__Exponential Family__<br/>
Generalized Linear Models(GLM)을 공부하기 전에 distribution들을 하나의 형태로 표현해주는 exponential family에 대해 알아보도록 하겠습니다.<br/>
다루는 data 혹은 수행해야하는 task의 종류에 따라서 사용하는 distribution이 달라집니다.
* Real value -> Gaussian 
* Binary -> Bernoulli
* Count -> Poisson 
* $R^2$ -> Gamma, Exponential (양의 정수)
* Distribution -> Beta, Dirichlet <- Mostly show up in Bayesian ML or Bayesian Statistics <br/>

이러한 distribution들을 아래의 식으로 표현해서 나타낼 수 있습니다.<br/>
![image](https://user-images.githubusercontent.com/76681022/213844785-1db4d9fb-b041-4340-8650-a817a88c01ff.png)

먼저 (Bernoulli -> use binary model) 이것을 조작해서 위의 exponential family 형태로 만들 수 있는데 아래의 식을 참고하시면 됩니다.<br/>

![image](https://user-images.githubusercontent.com/76681022/213845321-d98b2261-498f-40b1-8286-bd4e62272098.png)

위의 식을 보면 exponential family에 해당하는 값이 아래의 식처럼 구해질 수 있다는 것을 알 수 있습니다.<br/>
![image](https://user-images.githubusercontent.com/76681022/213845331-172edf8e-2aef-417f-889d-00c3b984f174.png)

이제는 Gaussian distribution도 exponential family로 표현해보도록 하겠습니다.<br/>
Gaussian distribution를 계산할 때는 variance = 1로 fix해주고 계산해보도록 하겠습니다.<br/>
![image](https://user-images.githubusercontent.com/76681022/213845370-2ed49cdc-dca3-4da9-9da5-c78f113c4ba2.png)
![image](https://user-images.githubusercontent.com/76681022/213845375-b3029ac1-34f2-4850-b086-f94298280e35.png)

Bernoulli와 마찬가지로 Gaussian 또한 exponential family로 나타낼 수 있음을 알 수 있습니다.<br/>

그런데 왜 exponential family를 사용해서 쓸까요?<br/>
그 이유는 exponential family가 mathmatical properties를 가지고 있기 때문입니다.<br/>
* Exponential family가 natural parameters로 parameterized되면
  * MLE with respect to eta -> concave (위로 볼록한 것)
  * Negative Log Likelihood is convex (아래로 볼록한 것)
* $E[y;\eta] =  {\partial\over\partial \eta}a(\eta)$
* $Var[y;\eta] = {\partial^2\over\partial eta^2}a(\eta)$
=> 적분을 안하고 미분만으로도 평균과 분산을 구할 수 있어서 계산이 간단해지기 때문입니다.<br/>
<br/>

__Generalized Linear Models(GLM)__<br/>
이제 GLM에 대해 알아보도록 하겠습니다.<br/>
GLM을 만들기위해서는 3가지 단계를 거쳐야 합니다.<br/>
1. y | x;θ ∼ ExponentialFamily(η). ( I.e., given x and θ, the distribution of y follows some exponential family distribution, with parameter η).<br/>
2. Given x, our goal is to predict the expected value of T(y) given x. In most of our examples, we will have T(y) = y, so this means we would like the prediction h(x) output by our learned hypothesis h to satisfy h(x) = E[y|x] => $h_{\theta}(x)$ = E[y|x;θ].<br/>
3. The natural parameter η and the inputs x are related linearly: η = $θ^T$ x. (Or, if η is vector-valued, then $\eta_{i} = \theta_{i}^Tx)$<br/>

이렇게 3가지의 단계로 설명하면 추상적이므로 실제 예시를 들어서 설명해보도록 하겠습니다.<br/>

- 그림 -
<br/>

Learning Update Rule : $\theta_{j} = \theta_{j} + \alpha (y^i - h_{\theta} (x^i))x_{j}^i$의 $h _{\theta}(x)$에 해당 data에 맞는 distribution 값을 넣고 나온 값을 이용하여 model을 update 해줄 수 있습니다.<br/>
<br/>
model param($\theta$) --$\theta^T x$--> natural param($\eta$) --g--> or <--$g^-1$-- canonical param($\phi, \mu, \sigma, \lambda$)<br/>
                                                
  <br/>
3-parameterizations 으로 정리할 수 있습니다. <br/>
1. Model parameter -> θ
2. Natural parameter -> $\mu$
3. Canonical parameter -> $\phi - Bernoulli, \mu,\sigma - Gaussian, \lambda - Poisson$<br/>
GLM을 학습하려면 model param만 학습시켜주면 된다는 것을 유의하면 되겠습니다.<br/>
<br/>
  
Logistic Regression의 경우, $h_{\theta}(x) = E[y|x; \theta] = \phi$가 됩니다. (Binary classification의 경우는 Bernoulli distribution을 사용하므로)<br/>
강의에서 Regression의 경우 그래프에서 데이터를 0,1을 구분하는 직선이 그어져 있는데 그 data가 위치하는 지점마다 gaussian distribution이 그어지게 되고 그 직선이 mean이 되게 됩니다. -> 직접 그림으로 보니까 좀 더 이해가 수월했습니다.<br/>
(여기서 제가 말하는 직선은 $\theta^T x$의 직선으로 해당 직선을 기준으로 위, 아래로 0,1이 구분되어지게 하는 직선입니다.)<br/>
<br/>
  
강의의 마지막으로는 Soffmax Regression(Cross Entropy minimization)을 non-GLM을 사용해서 multiclass classification을 보여줄 것입니다.<br/>
(k = number of classes, x $\in R^n$,  y를 [$ {0,1}^k$]로 one-hot vector를 이용해서 보여줄 것임.<br/>
softmax regression은 NxK matrix로 $\theta$가 class별로 존재합니다.<br/>
1:17의 그림으로 설명.<br/>
마지막에 p(k)와 one-hot vector를 distribution으로 바꿔주고 싶은 것<br/>
이게 두개의 distribution 사이의 cross entropy를 최소화해준다는 것.<br/>

