---
layout: post
title: CS229 Lecture3 정리
use_math: true
---

# CS229 Lec 3 정리   

### Lec 3 요약
Linear Regression으로는 해결이 되지 않을 때 사용하는 Locally weighted regression <br/>
Probablistic interpretation (Logistic regression, Newton's Method)에 대해서 알아 볼 예정입니다. <br/>


### Locally Weighted regression   
__Parametric learning algorithm__   
Fixed, finite number of parameters $\theta$<br/>
data가 computer memory에 저장되어 있지 않음.<br/>
=> 한 번 training을 하고나면 더 이상 training data에 접근하여 예측 값을 만들려고 하지 않음.<br/>
(여태까지 설명했던 것이 이것에 해당됨.) <br/>

<br/>
__Non-parametric learning algorithm__   
amount of data (parameters you need to keep) grows (linearly) with size of data   
data가 computer memory에 저장되어 있음.<br/>
=> 전체 training set을 계속 사용함. (hypothesis h grows linearly with the size of the training set.) <br/>
(Locally weighted regression이 여기에 해당됨.)
<br/>

__Locally weighted regression(LWR)__<br/>
data가 깔끔하게 직선으로 해결이 되지 않는 경우에 linear regression을 사용하면 underfitting, overfitting의 문제가 생길 수 있으므로 <br/>
LWR을 사용하게 된다면 예측 값을 보다 올바르게 찾아줄 수 있습니다.<br/>
linear regression에서 최소화하려고 했던 식에서 $W^i$만 추가한 식을 최소화해주면 LWR이 최소화해야하는 것을 찾을 수 있습니다.<br/>
식은 아래와 같습니다.<br/> 

<img width="146" alt="image" src="https://user-images.githubusercontent.com/76681022/213422899-8cfa058f-d9ea-4675-b704-98bafb66070b.png">
<br/>

해당 식에서 $W^i$에 대해 알아보도록 하겠습니다.<br/>
$W^i$는 weights라고 불리며 아래와 같은 식으로 구성되어 있습니다.<br/>
아래의 식의 의미에 대해 자세히 살펴보면 $x^i$ 즉 i번째 training sample에서 예측이 성공적으로 이루어져 실제 값과 차이가 얼마 나지 않으면 $W^i$는 1이 되고, 
예측이 좋지 못해서 오차가 크게 나게된다면 $W^i$가 0이 나오게 됩니다.<br/>
$W^i$가 최소화해야하는 식에 들어가있는데, weights의 의미를 생각해보면 유의미한 결과가 나온 값은 학습에 포함시키고 좋지 않은 결과가 나온 것은 포함시키지 않는 것임을 생각해볼 수 있습니다.<br/>
$\tau$는 

<img width="182" alt="image" src="https://user-images.githubusercontent.com/76681022/213424085-875be57b-3c79-472e-b78d-60fffb37ee90.png">
<br/>

- - -


Why we used the Squared Error <- 나중에 설명 <br/>
집 값을 예측한다고 예시를 들어보자.<br/>
y(i) = θTx(i) + ǫ <- random noise<br/>

epsilon is IID(Independently and Identically Distributed)라고 가정 <- IID하다는 뜻은 만약 예를 들어서
한 집의 가격이 높다고 해서 다른 집의 가격에 영향을 주지는 않는다는 뜻이다.
p(y|x) = ~~~ <- 한 집의 집 값을 예측하는 것은 theta*X에다가 noise epsilon을 더해주는 것
왜 입실론이 가우시안이냐? => Central Limit Theorem을 사용해주었기 때문,
X_(i);theta에서 ';' 을 쓰는 이유는 적혀져있는 것이 parameterized by theta라는 것을 의미함.
theta is not a random variable, theta is a set of parameters that parameterizes this  probability distribution
<br/>

likelihood of theta L(theta) = p(y|x;theta)
ㅠ = product.. error <- iid
probability of all of the observations ,of all the values of y in your training set is 
equal to the product of the probabilities (앞에서 independent하다고 가정을 해주었기 때문.)
data fixed, varing parameters = likelihood / varing data, fixed data = probability
likelihood of the parameters / probability of the data<br/>

MLE (Maximum Likelihood Estimation) : choose theta to maximizing L(theta)
l(theta) = log*L(theta) <- maximize해주려면 마지막에 있는 theta가 있는 항을 가장 작게 해줘야함.
1/2~~~ <- 줄여야 하는 항은 J(theta) = cost function

위에 있는 것을 사용해서 y = 0, 1이라고 해주면 이게 classification 문제이다.
근데 선형회귀를 분류 문제에서 잘 사용하지 않는다. 그림에서 보여준 예시와 같은 경우 때문에.
분류는 y가 0보다 작거나 1보다 크게 나오면 이상한거임. 그래서 분류에선 logistic regression을 쓴다.

theta_0 + theta_1*X = h_theta(X) e [0,1]이 나오기를 원함
g(z) = sigmoid or logistic function
logistic regression은 linear regression에서의 hypothesis가 나오게 된 값이 sigmoid function을 지나게 되면 0,1이 나오게 된다.
왜 sigmoid를 쓰냐? 나중에 더 많이 나옴.
p(y=1|x;theta) = h_theta(x) <- input을 넣으면 원하는 값이 나오게 된다.
p(y=0|x;theta) = 1 - h_theta(x) ex) cancer is benign or malignant
<위의 형식은 관습처럼 여겨지는 것임, h(x) / 1 - h(x)의 자리를 바꿔서 적어도 된다.>
p(y|x;theta) = h(x)^y*(1-h(x))^1-y <- y = 1이면 1이 나오고 0이면 0이 나오기 때문임. 항이 하나씩 사라짐 y의 값에 따라서

MLE
L(theta) = P(y|x;theta)
log likelihood에서 log*L(theta)를 가장 크게 만들어주기 위해서는 l(theta)를 maximize해주는 theta를 고르는 것이다. 
Batch gradient ascent를 써서 크게 만들어줄 것이다. gradient descent에서 부호만 바뀜.(climb up)
logistic function을 쓰는 이유 one global maximum을 보장해주기 때문. (로컬 미니멈은 없다고 가정)


__Newton's Method__
gradient ascent는 조금씩 커져가는데 Newton's Method는 훨씬 크게 증가한다. 
장단점 비교보다는 소개와 이게 어떤 것인가 하는 입장에서 보자
가끔은 위의 방법들보다 더 빠르기도 함.
function f가 있다.
f(theta) = 0가 되는 세타를 찾고싶음. <- 이게 뉴턴방법
we want maximize l(theta) = l'(theta) = 0
theta_0에서 접선을 그리고 이 접선이 0에 도달하는 지점에서 다음 theta를 잡는다 (theta_1)
그리고 theta_1에서 다시 접선을 그려준다. 이렇게 점점 반복해준다. 목표지점을 향해서...
quadratic convergence라고도 함.
error = 0.01 -> 0.00001 -> 0.00000001 error가 single iteration만으로도 제곱이 된다.
하지만 newton's method의 단점은 theta가 high dimentional vector인 경우 H(~)가 n+1*n+1이므로
큰 행렬을 계산해야하므로 안좋다.

<br/>



__Probablistic Interpretation__<br/>


