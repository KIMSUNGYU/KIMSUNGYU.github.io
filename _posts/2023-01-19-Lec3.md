---
layout: post
title: CS229 Lecture2 정리
use_math: true
---

# CS229 Lec 3 정리   
### Locally Weighted regression   
__Parametric learning algorithm__   
Fit fixed set of parameters to theta<br/>
data가 computer memory에 저장되어 있지 않음.<br/><br/>
__Non-parametric learning algorithm__   
amount of data(parameters you need to keep) grows (linearly) with size of data   
data가 computer memory에 저장되어 있음.<br/><br/>

Locally Weighted Regression<br/>
fit theta to minimize ~~~
여기서부터는 아이패드에 캡쳐한 식을 정리해서 보여주고 살짝 요런식이다 하고 보여주고 넘어감
가우시안 식 보여주면서<br/> 

Why we used the Squared Error <- 나중에 설명 <br/>
집 값을 예측한다고 예시를 들어보자.<br/>
y(i) = θTx(i) + ǫ <- random noise<br/>
gaussian pdf에 대한 설명. ~~~ 적분하면 1이 됨.
epsilon is IID(Independently and Identically Distributed)라고 가정 <- IID하다는 뜻은 만약 예를 들어서
한 집의 가격이 높다고 해서 다른 집의 가격에 영향을 주지는 않는다는 뜻이다.
p(y|x) = ~~~ <- 한 집의 집 값을 예측하는 것은 theta*X에다가 noise epsilon을 더해주는 것
왜 입실론이 가우시안이냐? => Central Limit Theorem을 사용해주었기 때문,
X_(i);theta에서 ';' 을 쓰는 이유는 적혀져있는 것이 parameterized by theta라는 것을 의미함.
theta is not a random variable, theta is a set of parameters that parameterizes this  probability distribution
<br/>

likelihood of theta L(theta) = p(y|x;theta)
ㅠ = product.. error <- iid
probability of all of the observations ,of all the values of y in your training set is 
equal to the product of the probabilities (앞에서 independent하다고 가정을 해주었기 때문.)
data fixed, varing parameters = likelihood / varing data, fixed data = probability
likelihood of the parameters / probability of the data<br/>

MLE (Maximum Likelihood Estimation) : choose theta to maximizing L(theta)
l(theta) = log*L(theta) <- maximize해주려면 마지막에 있는 theta가 있는 항을 가장 작게 해줘야함.
1/2~~~ <- 줄여야 하는 항은 J(theta) = cost function

위에 있는 것을 사용해서 y = 0, 1이라고 해주면 이게 classification 문제이다.
근데 선형회귀를 분류 문제에서 잘 사용하지 않는다. 그림에서 보여준 예시와 같은 경우 때문에.
분류는 y가 0보다 작거나 1보다 크게 나오면 이상한거임. 그래서 분류에선 logistic regression을 쓴다.

theta_0 + theta_1*X = h_theta(X) e [0,1]이 나오기를 원함
g(z) = sigmoid or logistic function
logistic regression은 linear regression에서의 hypothesis가 나오게 된 값이 sigmoid function을 지나게 되면 0,1이 나오게 된다.
왜 sigmoid를 쓰냐? 나중에 더 많이 나옴.
p(y=1|x;theta) = h_theta(x) <- input을 넣으면 원하는 값이 나오게 된다.
p(y=0|x;theta) = 1 - h_theta(x) ex) cancer is benign or malignant
<위의 형식은 관습처럼 여겨지는 것임, h(x) / 1 - h(x)의 자리를 바꿔서 적어도 된다.>
p(y|x;theta) = h(x)^y*(1-h(x))^1-y <- y = 1이면 1이 나오고 0이면 0이 나오기 때문임. 항이 하나씩 사라짐 y의 값에 따라서

MLE
L(theta) = P(y|x;theta)
log likelihood에서 log*L(theta)를 가장 크게 만들어주기 위해서는 l(theta)를 maximize해주는 theta를 고르는 것이다. 
Batch gradient ascent를 써서 크게 만들어줄 것이다. gradient descent에서 부호만 바뀜.(climb up)
logistic function을 쓰는 이유 one global maximum을 보장해주기 때문. (로컬 미니멈은 없다고 가정)


__Newton's Method__
gradient ascent는 조금씩 커져가는데 Newton's Method는 훨씬 크게 증가한다. 
장단점 비교보다는 소개와 이게 어떤 것인가 하는 입장에서 보자
가끔은 위의 방법들보다 더 빠르기도 함.
function f가 있다.
f(theta) = 0가 되는 세타를 찾고싶음. <- 이게 뉴턴방법
we want maximize l(theta) = l'(theta) = 0
theta_0에서 접선을 그리고 이 접선이 0에 도달하는 지점에서 다음 theta를 잡는다 (theta_1)
그리고 theta_1에서 다시 접선을 그려준다. 이렇게 점점 반복해준다. 목표지점을 향해서...
quadratic convergence라고도 함.
error = 0.01 -> 0.00001 -> 0.00000001 error가 single iteration만으로도 제곱이 된다.
하지만 newton's method의 단점은 theta가 high dimentional vector인 경우 H(~~)가 n+1*n+1이므로
큰 행렬을 계산해야하므로 안좋다. 

<br/>



__Probablistic Interpretation__<br/>


