---
layout: post
title: CS229 Lecture5 정리
use_math: true
---


# CS229 Lec 6 정리

### Lec 6 요약 
* Naive Bayes (Laplace smoothing, Event models)
* Comments on apply ML
* SVM(Soft Vector Machine) intro <br/>
Lec 6를 시작하기전에 Lec 5 내용들을 숙지하고 오는 것이 좋겠습니다.<br/>
이번 강의에서는 이전에 다루었던 스팸 필터를 이어서 만들면서 Naive Bayes에 대해 알아볼 예정이기 때문입니다.<br/>

__Naive Bayes__<br/>
먼저 NIPS라는 ML학회에서 메일을 받게 될 것입니다.<br/>
NIPS라는 학회에는 35,000개의 단어가 포함되어 있습니다.<br/>
그렇게 되면 그 단어가 포함되어있는 메일은 스팸메일로 분류하면 안되겠죠?<br/>
그래서 그 단어들이 포함되어있는 메일이 스팸메일일 확률은 0이 되는 것입니다.<br/>

1번 식 캡쳐해서 보여주기

하지만 이렇게 하면 결국 아래의 식과 같이 $0/0+0$의 형태가 되므로 statistically bad idea가 됩니다.<br/>
따라서 Laplace smoothing을 사용하여 스팸 필터를 만들 것입니다. <br/>

__Laplace smoothing__<br/>
Laplace smoothing이 어떤 것인지 이해하기 쉽게 간단하게 스포츠 경기 결과를 예시로 들어보겠습니다.<br/>
예를 들어 어떤 스포츠 팀이 4경기를 진행해서 4경기 모두 패배하였다고 가정해보겠습니다.<br/>
그랬을 때, 5번째 경기를 진행했을 때 해당 팀이 승리할 확률을 구하라고 하였을 때 일반적인 경우는 {이길 확률 = 이긴 경기 수 / (이긴 경기 수 + 진 경기 수)} 로 구하는데, Laplace smoothing을 사용하면 이긴 경기 수, 진 경기 수 둘 다 1씩 더해줘서 최종적으로 {이길 확률 = 이긴 경기 수 +1 / 이긴 경기 수 +1 + 진 경기 수 + 1}이 됩니다.<br/>
Laplace smoothing을 Naive Bayes에 적용해주면 아래의 식처럼 됩니다.<br/>

2번 식 캡쳐해서 보여주기

위의 방법으로 집 평수에 따라서 30일 뒤에 집이 팔리는 지 예측하는 classification 문제를 해결해보겠습니다.<br/>
집 평수에 따라서 4개의 값으로 나눠서 예측을 해보겠습니다.<br/>
그러면 $p(x|y) = /sum_{j=1}^n p(x_{j}|y)$가 됩니다.<br/>

