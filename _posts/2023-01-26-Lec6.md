---
layout: post
title: CS229 Lecture6 정리
use_math: true
---


# CS229 Lec 6 정리

### Lec 6 요약 
* Naive Bayes (Laplace smoothing, Event models)
* Comments on apply ML
* SVM(Soft Vector Machine) intro <br/>
Lec 6를 시작하기전에 Lec 5 내용들을 숙지하고 오는 것이 좋겠습니다.<br/>
이번 강의에서는 이전에 다루었던 스팸 필터를 이어서 만들면서 Naive Bayes에 대해 알아볼 예정이기 때문입니다.<br/>

__Naive Bayes__<br/>
먼저 NIPS라는 ML학회에서 메일을 받게 될 것입니다.<br/>
NIPS라는 학회에는 35,000개의 단어가 포함되어 있습니다.<br/>
그렇게 되면 그 단어가 포함되어있는 메일은 스팸메일로 분류하면 안되겠죠?<br/>
그래서 그 단어들이 포함되어있는 메일이 스팸메일일 확률은 0이 되는 것입니다.<br/>

1번 식 캡쳐해서 보여주기

하지만 이렇게 하면 결국 아래의 식과 같이 $0/0+0$의 형태가 되므로 statistically bad idea가 됩니다.<br/>
따라서 Laplace smoothing을 사용하여 스팸 필터를 만들 것입니다. <br/>

__Laplace smoothing__<br/>
Laplace smoothing이 어떤 것인지 이해하기 쉽게 간단하게 스포츠 경기 결과를 예시로 들어보겠습니다.<br/>
예를 들어 어떤 스포츠 팀이 4경기를 진행해서 4경기 모두 패배하였다고 가정해보겠습니다.<br/>
그랬을 때, 5번째 경기를 진행했을 때 해당 팀이 승리할 확률을 구하라고 하였을 때 일반적인 경우는 {이길 확률 = 이긴 경기 수 / (이긴 경기 수 + 진 경기 수)} 로 구하는데, Laplace smoothing을 사용하면 이긴 경기 수, 진 경기 수 둘 다 1씩 더해줘서 최종적으로 {이길 확률 = 이긴 경기 수 +1 / 이긴 경기 수 +1 + 진 경기 수 + 1}이 됩니다.<br/>
Laplace smoothing을 Naive Bayes에 적용해주면 아래의 식처럼 됩니다.<br/>

2번 식 캡쳐해서 보여주기

위의 방법으로 집 평수에 따라서 30일 뒤에 집이 팔리는 지 예측하는 classification 문제를 해결해보겠습니다.<br/>
집 평수에 따라서 4개의 값으로 나눠서 예측을 해보겠습니다.<br/>
그러면 $p(x|y) = /sum_{j=1}^n p(x_{j}|y)$가 됩니다.<br/>

하지만 이것보다 더 쉬운 예시로 스팸메일을 필터로 거르는 것을 예시로 들어서 설명해보겠습니다.<br/>
Multivariate Bernoulli model과 Multinomial event model을 사용해서 해결해보겠습니다.<br/>

~~ 강의 설명 듣고 위의 두 모델 식 이해하고 그거 정리해서 포스팅하기.

24 ~ 29분 강의 내용.

$\phi_{k|y=0}$ = 스팸 메일이 아닌 경우에 그 메일에 k번째 단어(특정 단어)가 얼마나 들어가있는지를 나타내주는 것<br/>
3,4번 캡쳐해서 보여주고 식에 해당하는 의미 작성해주기.
3 -> laplace smoothing -> 4가 되는 것.

분모에는 |v|를 더해주는 것은 dictionary에 단어들이 몇개 들어있는지를 더해주는 것임.<br/>
단어가 얼마나 나올 수 있느냐를 의미하기 때문?<br/>
Naive Bayes보다 logistic regression이 더 성능이 좋지만 Naive Bayes가 계산이 간단하고 좀 더 실행하기에 더 빠르므로 사용하는 이유가 있음.<br/>
결론적으로 우리는 GDA, Naive Bayes를 배웠는데 이 두가지 방법은 결코 정확한 예측을 보이지 않음.<br/>
하지만 quick and dirty라는 특성을 가지기 때문에, 좀 더 계산이 빠르고 iteration을 통해서 예측을 하는 것이 아니므로 단순 계산을 통해 예측을 할 수 있고, 실행하기가 쉬운 특성을 가지고 있음.<br/>
좀 더 정확한 algorithm을 사용하려면 logistic regression, SVM, neural net을 사용하면 됨.<br/>
우리가 사용하려는 목적에 맞게 algorithm을 설정해서 사용해주면 된다고 말씀해주셨습니다.<br/>


__SVM__ <br/>
SVM은 non-linear decision boundary의 문제를 해결할 때 사용되고 많은 parameter가 필요하지 않은 것이 장점입니다.<br/>

SVM은 이러한 특징이 있고 Optimal margin classifier, functional margin, geometric margin과 함께 추가설명은 Lec 6, Lec 7에 이어서 설명이 되어있으므로 좀 더 강의를 듣고 Lec 7에서 정리하여 업로드하도록 하겠습니다.<br/>

