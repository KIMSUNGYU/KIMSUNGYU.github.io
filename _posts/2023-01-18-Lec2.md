---
layout: post
title: CS229 Lecture1 정리
use_math: true
---

# CS229 Lec 2 정리 
## __강의를 정리하기 전에.__
공부를 함에 있어서 중요한 것은 선대의 학자들이 정리하고 그렇게 쓰기로 약속한 기호, 문자, 표기법등을 숙지하여 수식을 공부하고 이해하는 것이 중요하다고 생각합니다.<br/>
앞으로 강의를 정리함에 있어 다양한 문자, 기호들이 나오는데 이것을 잘 받아들여서 공부를 하면 좋을 것 같습니다.

### __Lec 2 요약__
Linear Regression이란 n개의 features를 가진 m개의 training set을 가지고 learning algorithm을 거쳐서 $\hat{y}$ (output = predicted y)을 찾아주는 것입니다.<br/>
cost function $J(\theta)$를 활용하여 predicted y와 실제 data y의 차이를 최소화하도록 할 것입니다.<br/>
cost function $ J(\theta) $에서 예측 값과 실제 값의 차이를 줄이기 위해 사용하는 방법이 Gradient Descent 방법인데, Batch Gradient Descent 와 Stochastic Gradient Descet (SGD)를 사용할 수 있습니다.<br/>
이번 강의 정리에서는 위의 내용을 정리하도록 하겠습니다.<br/>
<br/>

### __Linear Regression__
*Machine Learning에서 input X는 대문자, output y는 소문자로 표기하므로 유의하시면 좋겠습니다.*<br/>
먼저 Linear Regression에서 $ \theta $는 parameters (weights)라고 하고 X를 y에 mapping 시킬 때 사용됩니다.<br/>

<br/>
$h(x) = \theta_{0} + \theta_{1}*X_{1} + \theta_{2}*X_{2}$ <br/>
<br/>
(여기서 $\theta_{0}$에 $X_{0}$가 붙지 않은 이유는 $X_{0} = 1$로 dummy feature이기 때문입니다.)<br/>
먼저 hypothesis는 가정이라는 뜻인데, 제 나름대로 추측하기로는 input X 값을 활용해서 우리가 원하는 output y의 값을 구할 것이므로 사용하지 않았나하는 생각입니다.<br/>
Lec1에서의 예시를 계속 사용해보겠습니다. 방의 개수등 집 값에 영향을 주는 input features를 X라고 하고, 우리가 구하길 원하는 집 값을 output y라고 했을 때, 우리가 구한 예측 값은 $h_{\theta}(X^i)$ = $\hat{y}$가 되는 것입니다.<br/>
m개의 모든 training sample에 대해서 나온 $\hat{y}$ 값은 h(X)가 됩니다.<br/>
$$

